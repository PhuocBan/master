{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "build_word2vec_vnexpress.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDmkn2vMLeR1",
        "outputId": "700a3395-650e-4489-d4ae-06a854fc082f"
      },
      "source": [
        "from google.colab import drive   \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clG1hs0SFHVm"
      },
      "source": [
        "# Tiền xử lí dữ liệu VnExpress thu thập được"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmzyhEurYXhO"
      },
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "-PGxGGe2YiBL",
        "outputId": "7c589640-a8d2-449c-c826-3700edb167ca"
      },
      "source": [
        "# Tiền xử lí file dữ liệu thu thập từ vnexpress\n",
        "data = open('/content/drive/My Drive/code_crawl/vnexpress/raw.txt', 'r', encoding='utf8').readlines()\n",
        "for i in range(len(data)):\n",
        "  data[i] = BeautifulSoup(data[i], \"lxml\").text\n",
        "  data[i] = data[i].replace('\\xa0', ' ')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://vietnamese.vietnam.usembassy.gov/yseali-2016.html\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://gomhailong.vn/am-sac-thuoc-dien-tu-dong\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://gomhailong.vn\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://hailongtiles.com/\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.choray.vn\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.facebook.com/dieuphoigheptangvietnam\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.golfdigest.com/story/rory-mcilroy-grinding-in-the-pro-amto-win-a-bet-and-stay-focused\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.golf.com/instruction/bunker-shots/2020/03/04/rory-mcilroy-bunker-bay-hill-putt/\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.facebook.com/VnExpressInternational/\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://twitter.com/int_vn\n",
            "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YyV5AWsYR0u"
      },
      "source": [
        "# loại bỏ những câu quá ngắn\n",
        "data1 = []\n",
        "for i in range(len(data)):\n",
        "  if len(data[i].split()) > 10:\n",
        "    data1.append(data[i])\n",
        "data = data1\n",
        "data1 = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1wGk3agyIJ9"
      },
      "source": [
        "import re\n",
        "# tiền xử lí các số, email, website\n",
        "for i in range(len(data)):\n",
        "  # chuyển các con số thành NUMBER\n",
        "  data[i] = re.sub(\"\\d+\", \"NUMBER\", data[i])\n",
        "  # chuyển các địa chỉ email thành EMAIL\n",
        "  data[i] = re.sub(\"/\\S+@\\S+\\.\\S+/\", \"EMAIL\", data[i])\n",
        "  # chuyển các địa chỉ website thành URL\n",
        "  data[i] = re.sub(\"((\\w+:\\/\\/\\S+)|(\\w+[\\.:]\\w+\\S+))[^\\s,\\.]\", \"URL\", data[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OrXQ0-i837F"
      },
      "source": [
        "# Sau khi loại thay thế các địa chỉ mail, website,... ta tiến hành xử lí các dấu câu và khoảng trắng dư thừa\n",
        "data1 = []\n",
        "for i in data:\n",
        "  temp = i.split('.')\n",
        "  for j in temp:\n",
        "    if len(j.split()) >8:\n",
        "      temp1 = re.sub('''[,#!\\^&\\*;:{}=`?\\[\\]'\"“”‘’~()]''', \" \", j)\n",
        "      temp1 = re.sub(\"\\s+\", \" \", temp1)\n",
        "      temp1 = temp1.strip()\n",
        "      temp1 = temp1.lower()\n",
        "      data1.append(temp1)\n",
        "data = data1.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKu7kYJ6fdDy"
      },
      "source": [
        "# ghi lại tập dữ liệu sau khi đã được tiền xử lí\n",
        "f = open('/content/drive/My Drive/code_crawl/vnexpress/text_preprocessor1.txt', 'w', encoding='utf8')\n",
        "for i in data:\n",
        "  f.write(i+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15mrsDwHBbxe"
      },
      "source": [
        "# Tạo model với tập dữ liệu từ vnexpress đã tiền xử lí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7Px7LmbCgsg"
      },
      "source": [
        "## Mô hình cbow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcTrnXs7BiNe"
      },
      "source": [
        "# tạo mô hình với dữ liệu bình thường\r\n",
        "from gensim.models import Word2Vec\r\n",
        "data_input = '/content/drive/My Drive/code_crawl/vnexpress/text_preprocessor1.txt'\r\n",
        "model = Word2Vec(corpus_file=data_input, vector_size=300, window=4, min_count=5, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKMvJtwYBiQX"
      },
      "source": [
        "# lưu mô hình cbow lại\r\n",
        "import pickle\r\n",
        "pickle.dump(model, open('/content/drive/My Drive/Colab Notebooks/word2vec/cbow/model.bin', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FUvcgPxDC1k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqY7CNLaDFPW"
      },
      "source": [
        "## Mô hình skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cxh1prWDFPf"
      },
      "source": [
        "# tạo mô hình với dữ liệu bình thường\r\n",
        "from gensim.models import Word2Vec\r\n",
        "data_input = '/content/drive/My Drive/code_crawl/vnexpress/text_preprocessor1.txt'\r\n",
        "model = Word2Vec(corpus_file=data_input, vector_size=300, window=4, min_count=5, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Yjk8INDFPf"
      },
      "source": [
        "# lưu mô hình cbow lại\r\n",
        "import pickle\r\n",
        "pickle.dump(model, open('/content/drive/My Drive/Colab Notebooks/word2vec/skip-gram/model.bin', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqSwh4uBt6IE"
      },
      "source": [
        "# Tạo tập corpus, small corpus, badword phục vụ sửa lỗi chính tả và tạo dữ liệu train cho mô hình\n",
        "- Phần này chỉ thao tác trên phần dữ liệu train của HateSpeech\n",
        "- Corpus là tập từ vựng được tạo từ bộ từ vựng của vnexpress + từ vựng bộ train\n",
        "- Small corpus là tập từ vựng bộ train\n",
        "- Badword ở đây bao gồm những từ xuất hiện nhiều trong lớp hate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_QNM4zK9v5z"
      },
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITDqjLDYxgGX"
      },
      "source": [
        "dir_folder = '/content/drive/My Drive/Colab Notebooks/dump_hatespeech/'\n",
        "def load(file_name, dir_folder):\n",
        "  f = open(dir_folder + file_name, 'rb')\n",
        "  variable = pickle.load(f)\n",
        "  f.close()\n",
        "  return variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGBBcDNQApJP"
      },
      "source": [
        "# đọc dữ liệu từ phần train của hate speech\n",
        "train_x = load('train_x.dump', dir_folder)\n",
        "train_y = load('train_y.dump' , dir_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da_mtAIFEPcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b889c4-a21d-4896-eb11-b36ebf462878"
      },
      "source": [
        "print('X length: ',len(train_x))\n",
        "print('y length:', len(train_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X length:  16276\n",
            "y length: 16276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PigOkJBUz2CG"
      },
      "source": [
        "Xử lí dữ liệu tập train_x\n",
        "- Thay thế các kí tự giống nhau lặp lại nhiều lần\n",
        "- Thay thế đoạn html còn sót lại \\x01\n",
        "- Thay thế dấu gạch và các khoảng trắng dư thừa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE58Pre20Cvp"
      },
      "source": [
        "import re \n",
        "train_x = list(train_x)\n",
        "temp = []\n",
        "for index, i in enumerate(train_x):\n",
        "  i = re.sub(r'(.)\\1{2,}', r'\\1\\1', i)\n",
        "  i = re.sub('\\x01', '', i)\n",
        "  i = re.sub('_|\\s+', ' ', i)\n",
        "  train_x[index] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TUrYGb7JPHY"
      },
      "source": [
        "## Thống kê tần suất của từng lớp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "935n6ZOHJSAX"
      },
      "source": [
        "# hàm thống kê tần suất kí tự\n",
        "from collections import Counter\n",
        "def word_statistics(sent_list):\n",
        "  \"\"\"\n",
        "  input: \n",
        "    sent_list: list include sents\n",
        "  ouput: \n",
        "    words: list include words and freq\n",
        "    log: text include lines, each line is <word> <freq>\n",
        "  \"\"\"\n",
        "  data = ' '.join(sent_list)\n",
        "  c = Counter(data.split())\n",
        "  words = []\n",
        "  for i in list(c):\n",
        "    words.append([i, c[i]])\n",
        "  def freq(w):\n",
        "    return w[1]\n",
        "  # sort danh sách từ lại\n",
        "  words.sort(key=freq, reverse=True)\n",
        "  # tạo thành 1 file văn bản thống kê tần suất các từ\n",
        "  log = ''\n",
        "  for i in words:\n",
        "    log = log + i[0] + ' ' + str(i[1]) + '\\n'\n",
        "  return words, log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fo2FGWT6vXw0",
        "outputId": "f8ec1cb3-eba6-48e8-ec3e-9f0ce7245bf3"
      },
      "source": [
        "'|'.join('àáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýÀÁÃẠẢĂẮẰẲẴẶÂẤẦẨẪẬÈÉẸẺẼÊỀẾỂỄỆĐÌÍĨỈỊÒÓÕỌỎÔỐỒỔỖỘƠỚỜỞỠỢÙÚŨỤỦƯỨỪỬỮỰỲỴỶỸÝ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'à|á|ã|ạ|ả|ă|ắ|ằ|ẳ|ẵ|ặ|â|ấ|ầ|ẩ|ẫ|ậ|è|é|ẹ|ẻ|ẽ|ê|ề|ế|ể|ễ|ệ|đ|ì|í|ĩ|ỉ|ị|ò|ó|õ|ọ|ỏ|ô|ố|ồ|ổ|ỗ|ộ|ơ|ớ|ờ|ở|ỡ|ợ|ù|ú|ũ|ụ|ủ|ư|ứ|ừ|ử|ữ|ự|ỳ|ỵ|ỷ|ỹ|ý|À|Á|Ã|Ạ|Ả|Ă|Ắ|Ằ|Ẳ|Ẵ|Ặ|Â|Ấ|Ầ|Ẩ|Ẫ|Ậ|È|É|Ẹ|Ẻ|Ẽ|Ê|Ề|Ế|Ể|Ễ|Ệ|Đ|Ì|Í|Ĩ|Ỉ|Ị|Ò|Ó|Õ|Ọ|Ỏ|Ô|Ố|Ồ|Ổ|Ỗ|Ộ|Ơ|Ớ|Ờ|Ở|Ỡ|Ợ|Ù|Ú|Ũ|Ụ|Ủ|Ư|Ứ|Ừ|Ử|Ữ|Ự|Ỳ|Ỵ|Ỷ|Ỹ|Ý'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22MXiYa4KaPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dd9092-54cf-458c-d20e-7613e9c3ed8d"
      },
      "source": [
        "import re\n",
        "# tách riêng các câu của từng lớp ra\n",
        "clean_sents = []\n",
        "offensive_sents = []\n",
        "hate_sents = []\n",
        "dau = 'à|á|ã|ạ|ả|ă|ắ|ằ|ẳ|ẵ|ặ|â|ấ|ầ|ẩ|ẫ|ậ|è|é|ẹ|ẻ|ẽ|ê|ề|ế|ể|ễ|ệ|đ|ì|í|ĩ|ỉ|ị|ò|ó|õ|ọ|ỏ|ô|ố|ồ|ổ|ỗ|ộ|ơ|ớ|ờ|ở|ỡ|ợ|ù|ú|ũ|ụ|ủ|ư|ứ|ừ|ử|ữ|ự|ỳ|ỵ|ỷ|ỹ|ý|À|Á|Ã|Ạ|Ả|Ă|Ắ|Ằ|Ẳ|Ẵ|Ặ|Â|Ấ|Ầ|Ẩ|Ẫ|Ậ|È|É|Ẹ|Ẻ|Ẽ|Ê|Ề|Ế|Ể|Ễ|Ệ|Đ|Ì|Í|Ĩ|Ỉ|Ị|Ò|Ó|Õ|Ọ|Ỏ|Ô|Ố|Ồ|Ổ|Ỗ|Ộ|Ơ|Ớ|Ờ|Ở|Ỡ|Ợ|Ù|Ú|Ũ|Ụ|Ủ|Ư|Ứ|Ừ|Ử|Ữ|Ự|Ỳ|Ỵ|Ỷ|Ỹ|Ý'\n",
        "for index, i in enumerate(train_y):\n",
        "  n = 0\n",
        "  for j in train_x[index].split():\n",
        "    if re.search(dau, j):\n",
        "      n += 1\n",
        "  if n < len(train_x[index].split())/3:\n",
        "    continue\n",
        "\n",
        "  if i[0] == 1:\n",
        "    clean_sents.append(train_x[index])\n",
        "  elif i[1] == 1:\n",
        "    offensive_sents.append(train_x[index])\n",
        "  else:\n",
        "    hate_sents.append(train_x[index])\n",
        "\n",
        "print(\"Clean length    \\t\", len(clean_sents))\n",
        "print(\"Offensive length\\t\", len(offensive_sents))\n",
        "print(\"Hate length     \\t\", len(hate_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean length    \t 13672\n",
            "Offensive length\t 751\n",
            "Hate length     \t 526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uek8aUM7dYRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2afb92-20af-4469-d2fc-e0d05d3bcbfb"
      },
      "source": [
        "# Thống kê lần lượt 3 lớp clean, offensive và hate\n",
        "log_folder_dir = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/temp/word_statistics/'\n",
        "# clean\n",
        "clean_words, clean_log = word_statistics(clean_sents)\n",
        "open(log_folder_dir + 'clean_words.txt', 'w').write(clean_log)\n",
        "# offensive\n",
        "offensive_words, offensive_log = word_statistics(offensive_sents)\n",
        "open(log_folder_dir + 'offensive_words.txt', 'w').write(offensive_log)\n",
        "# hate\n",
        "hate_words, hate_log = word_statistics(hate_sents)\n",
        "open(log_folder_dir + 'hate_words.txt', 'w').write(hate_log)\n",
        "print(\"Finish statistics.\")\n",
        "print(\"Clean words length    \\t\", len(clean_words))\n",
        "print(\"Offensive words length\\t\", len(offensive_words))\n",
        "print(\"Hate words length     \\t\", len(hate_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish statistics.\n",
            "Clean words length    \t 12027\n",
            "Offensive words length\t 1953\n",
            "Hate words length     \t 2179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNbketddiXfr"
      },
      "source": [
        "Lấy ngưỡng từng bộ từ vựng\n",
        "- clean > 8\n",
        "- hate & offisive > 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fKIk3ALXik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350341f2-ac83-4d2d-9efc-efcbff79b4d2"
      },
      "source": [
        "def cut_threshold(words, threshold):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    words: (list) contain objects. each object is [word, freq]\n",
        "    threshold: (int) cut at the position where the frequency is greater than the threshold\n",
        "  \"\"\"\n",
        "  for index, i in enumerate(words):\n",
        "    if not (i[1] > threshold):\n",
        "      return words[:index]\n",
        "  print(\"Error did not return the list\")\n",
        "  return None\n",
        "\n",
        "#clean\n",
        "clean_words = cut_threshold(clean_words, 8)\n",
        "# offensive\n",
        "offensive_words = cut_threshold(offensive_words, 2)\n",
        "# hate\n",
        "hate_words = cut_threshold(hate_words, 2)\n",
        "\n",
        "print(\"Finish cut threshold\")\n",
        "print(\"Clean words length    \\t\", len(clean_words))\n",
        "print(\"Offensive words length\\t\", len(offensive_words))\n",
        "print(\"Hate words length     \\t\", len(hate_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish cut threshold\n",
            "Clean words length    \t 2945\n",
            "Offensive words length\t 645\n",
            "Hate words length     \t 822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f-tEkGrHi3P"
      },
      "source": [
        "## Tạo tập small corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3JTLcL_mJWK"
      },
      "source": [
        "Gộp các bộ từ vựng lại với nhau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q3EOqBSlv5m"
      },
      "source": [
        "def merge_vocab(*vocab_list, weight = None, isFreq = True):\n",
        "  if isFreq:\n",
        "    if weight != None:\n",
        "      result = dict(vocab_list[0])\n",
        "      for i in result:\n",
        "        result[i] = result[i]*weight[0]\n",
        "\n",
        "      for index, i in list(enumerate(vocab_list))[1:]:\n",
        "        i = dict(i)\n",
        "        for j in i:\n",
        "          try:\n",
        "            result[j] = result[j] + i[j]*weight[index]\n",
        "          except:\n",
        "            result[j] = i[j]*weight[index]\n",
        "      return result\n",
        "  else:\n",
        "    if weight == None:\n",
        "      result = set(vocab_list[0])\n",
        "      for i in vocab_list[1:]:\n",
        "        result = result | set(i)\n",
        "      return result\n",
        "  print(\"Error: incomplete function\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGTcNTHonRRl"
      },
      "source": [
        "small_corpus = merge_vocab(clean_words, hate_words, offensive_words, weight=[1,12,6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fed0--zDv5-H"
      },
      "source": [
        "# sort lại danh sách\n",
        "small_corpus = sorted(small_corpus.items(), key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osLX4uVeyKhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baecb946-ffaf-48f7-c17d-6bb593d30307"
      },
      "source": [
        "# ghi file small corpus xuống\n",
        "small_corpus_folder_dir = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/'\n",
        "s = ''\n",
        "for i in small_corpus:\n",
        "  # loại bỏ ác từ 1 kí tự hoặc thuộc 1 trong 11 loại phụ âm ghép\n",
        "  if len(i[0]) != 1 and i[0] not in ['ph', 'th', 'tr', 'gi', 'ch', 'nh', 'ng', 'kh', 'gh', 'ngh', 'qu']:\n",
        "    s = s + i[0] + ' ' + str(i[1]) + '\\n'\n",
        "s = s[:-1]\n",
        "open(small_corpus_folder_dir + 'small_corpus.txt', 'w').write(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiRt4yg56Mvi"
      },
      "source": [
        "## Tạo tập corpus refine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSELstxY0NjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0893ef48-dc40-4d4a-9c71-9d733c635819"
      },
      "source": [
        "small_corpus = dict(small_corpus)\n",
        "# đọc tập từ vựng của vnexpres\n",
        "def read_vocab_vne(dir_vocab_vnexpress, threshold):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    dir_vocab_vnexpress: the path of the vnexpress vocabulary file\n",
        "    threshold: (int) cut at the position where the frequency is greater than the threshold\n",
        "  Output:\n",
        "    (set) vocabulary\n",
        "  \"\"\"\n",
        "  temp = open(dir_vocab_vnexpress).read().split('\\n')[:-1]\n",
        "  vocab = []\n",
        "  for i in temp:\n",
        "    word, freq = i.split()\n",
        "    if int(freq) > threshold:\n",
        "      vocab.append(word)\n",
        "  return set(vocab)\n",
        "\n",
        "dir_vocab_vnexpress = '/content/drive/My Drive/code_crawl/vnexpress/vocab1.txt'\n",
        "corpus = read_vocab_vne(dir_vocab_vnexpress, 100)\n",
        "print(\"Vocabulary Vnexpress length: \", len(corpus))\n",
        "\n",
        "# merge corpus and corpus refine\n",
        "corpus = corpus | set(small_corpus)\n",
        "print(\"Corpus length:               \", len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Vnexpress length:  11815\n",
            "Corpus length:                12210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpvE54YQUIy"
      },
      "source": [
        "# merge thêm tập từ vựng tokenizer\n",
        "py_words = open('/content/drive/MyDrive/Colab Notebooks/KLTN_hatespeech/pyvi_words_edited.txt').read().split('\\n')\n",
        "corpus = corpus | set(py_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptjLPkne6jSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0cee0d-1072-4434-e056-d9754e6214c6"
      },
      "source": [
        "# ghi file corpus xuống\n",
        "dir_corpus_file = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/corpus.txt'\n",
        "open(dir_corpus_file, 'w').write('\\n'.join(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "295971"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx1oJp_UL8_5"
      },
      "source": [
        "Tạo tập vocab hate and offensive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOO8PCQz6KNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb63b811-d114-4bd8-d215-5416646721f1"
      },
      "source": [
        "hate_offensive_vocab = merge_vocab(hate_words, offensive_words, weight=[1,1])\n",
        "# sort lại danh sách\n",
        "hate_offensive_vocab = sorted(hate_offensive_vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "# ghi file small corpus xuống\n",
        "hate_offensive_vocab_dir = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/'\n",
        "open(hate_offensive_vocab_dir + 'hate_offensive_vocab1.txt', 'w').write('\\n'.join(dict(hate_offensive_vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZESqEg7HEsZ"
      },
      "source": [
        "## Tạo dữ liệu train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tPjnswrdP5u",
        "outputId": "06ae9236-1c9d-409a-cc5f-9fb03e8f15f5"
      },
      "source": [
        "# thống kê tần suất các lớp\n",
        "print(\"Clean sent length    \\t\", len(clean_sents))\n",
        "print(\"Offensive sent length\\t\", len(offensive_sents))\n",
        "print(\"Hate sent length     \\t\", len(hate_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean sent length    \t 13672\n",
            "Offensive sent length\t 751\n",
            "Hate sent length     \t 526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_EzPd9I8P1c"
      },
      "source": [
        "import random\n",
        "# tạo dữ liệu train\n",
        "data = clean_sents + hate_sents*12 + offensive_sents*12\n",
        "random.shuffle(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qHlLV838WQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c008988d-d306-4797-828f-ab003b06c22b"
      },
      "source": [
        "# ghi xuôgns thành file\n",
        "open('/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/fasttext_data/train.txt', 'w').write('\\n'.join(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2414481"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dYOrtvaDQ-e"
      },
      "source": [
        "# Turning lại dữ liệu theo miền hate speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVeQMqO0DW0F"
      },
      "source": [
        "## CBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJEGohKjDdmA"
      },
      "source": [
        "import pickle\r\n",
        "from gensim.models import Word2Vec\r\n",
        "# turning lại mô hình\r\n",
        "data_input = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/fasttext_data/train.txt'\r\n",
        "model = pickle.load(open('/content/drive/My Drive/Colab Notebooks/word2vec/cbow/model.bin', 'rb'))\r\n",
        "model.build_vocab(corpus_file=data_input, update=True)\r\n",
        "model.train(corpus_file=data_input, word_count=5, epochs=10, total_words=model.corpus_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgMdvkzZDtNB"
      },
      "source": [
        "# lấy tập từ vựng của mô hình\r\n",
        "# fasttext_vocab = model.get_words()\r\n",
        "fasttext_vocab = list(model.wv.vocab)\r\n",
        "# giao với tập corpus\r\n",
        "fasttext_vocab = set(fasttext_vocab) & corpus\r\n",
        "print(\"Finish get vocabulary from fasttext and merge corpus.\")\r\n",
        "print(\"Corpus length            \", len(corpus))\r\n",
        "print(\"Fasttext corpus lenght   \", len(fasttext_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOXsd8RID9VL"
      },
      "source": [
        "# lấy tập vector ứng với tập từ vựng trên\r\n",
        "vector = []\r\n",
        "for word in fasttext_vocab:\r\n",
        "  vector.append(model.wv.get_vector(word))\r\n",
        "  # vector.append(model.get_word_vector(word))\r\n",
        "print(\"Finish get vector from fasttext.\")\r\n",
        "print(\"vector length   \", len(fasttext_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_x7kWlbEIDs"
      },
      "source": [
        "Ghi dữ liệu xuống"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4HbbRlnECWK"
      },
      "source": [
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/KLTN_hatespeech/Model/cbow/'\r\n",
        "# ghi file vocab\r\n",
        "open(folder_path + 'vocab.txt', 'w').write('\\n'.join(fasttext_vocab))\r\n",
        "# ghi file vector\r\n",
        "vec = np.array(vector)\r\n",
        "np.save(folder_path + 'vec.npy', vec)\r\n",
        "print('Finish write vocabulary and vector.')\r\n",
        "print('Vector shape: ',vec.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdTuLqUDEPr1"
      },
      "source": [
        "## SKIP-GRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ITcDxrEEPr9"
      },
      "source": [
        "import pickle\r\n",
        "from gensim.models import Word2Vec\r\n",
        "# turning lại mô hình\r\n",
        "data_input = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/fasttext_data/train.txt'\r\n",
        "model = pickle.load(open('/content/drive/My Drive/Colab Notebooks/word2vec/skip-gram/model.bin', 'rb'))\r\n",
        "model.build_vocab(corpus_file=data_input, update=True)\r\n",
        "model.train(corpus_file=data_input, word_count=5, epochs=10, total_words=model.corpus_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCI_HtznEPr-"
      },
      "source": [
        "# lấy tập từ vựng của mô hình\r\n",
        "# fasttext_vocab = model.get_words()\r\n",
        "fasttext_vocab = list(model.wv.vocab)\r\n",
        "# giao với tập corpus\r\n",
        "fasttext_vocab = set(fasttext_vocab) & corpus\r\n",
        "print(\"Finish get vocabulary from fasttext and merge corpus.\")\r\n",
        "print(\"Corpus length            \", len(corpus))\r\n",
        "print(\"Fasttext corpus lenght   \", len(fasttext_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhjVbT4JEPr-"
      },
      "source": [
        "# lấy tập vector ứng với tập từ vựng trên\r\n",
        "vector = []\r\n",
        "for word in fasttext_vocab:\r\n",
        "  vector.append(model.wv.get_vector(word))\r\n",
        "  # vector.append(model.get_word_vector(word))\r\n",
        "print(\"Finish get vector from fasttext.\")\r\n",
        "print(\"vector length   \", len(fasttext_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuh8a3CEEPr-"
      },
      "source": [
        "Ghi dữ liệu xuống"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P9ZvcKPEPr_"
      },
      "source": [
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/KLTN_hatespeech/Model/skip-gram/'\r\n",
        "# ghi file vocab\r\n",
        "open(folder_path + 'vocab.txt', 'w').write('\\n'.join(fasttext_vocab))\r\n",
        "# ghi file vector\r\n",
        "vec = np.array(vector)\r\n",
        "np.save(folder_path + 'vec.npy', vec)\r\n",
        "print('Finish write vocabulary and vector.')\r\n",
        "print('Vector shape: ',vec.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PEesyWEkTS"
      },
      "source": [
        "## Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CmobpYNEkTf"
      },
      "source": [
        "!pip install fasttext\r\n",
        "import fasttext\r\n",
        "pre_model = '/content/drive/My Drive/Colab Notebooks/Hate Speech Detection/Data/cc.vi.300.vec'\r\n",
        "# train mô hình\r\n",
        "data_input = '/content/drive/My Drive/Colab Notebooks/KLTN_hatespeech/fasttext_data/train.txt'\r\n",
        "model = fasttext.train_supervised(input=data_input, epoch=10, lr = 0.1, dim=300, minCount=5, pretrainedVectors= pre_model )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TAE-3f2EkTg"
      },
      "source": [
        "# lấy tập từ vựng của mô hình\r\n",
        "fasttext_vocab = model.get_words()\r\n",
        "# fasttext_vocab = list(model.wv.vocab)\r\n",
        "# giao với tập corpus\r\n",
        "fasttext_vocab = set(fasttext_vocab) & corpus\r\n",
        "print(\"Finish get vocabulary from fasttext and merge corpus.\")\r\n",
        "print(\"Corpus length            \", len(corpus))\r\n",
        "print(\"Fasttext corpus lenght   \", len(fasttext_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRxMiDArEkTh"
      },
      "source": [
        "# lấy tập vector ứng với tập từ vựng trên\r\n",
        "vector = []\r\n",
        "for word in fasttext_vocab:\r\n",
        "  # vector.append(model.wv.get_vector(word))\r\n",
        "  vector.append(model.get_word_vector(word))\r\n",
        "print(\"Finish get vector from fasttext.\")\r\n",
        "print(\"vector length   \", len(fasttext_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vmjddJmEkTi"
      },
      "source": [
        "Ghi dữ liệu xuống"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ijtbQ6EkTi"
      },
      "source": [
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/KLTN_hatespeech/Model/fasttextfb/'\r\n",
        "# ghi file vocab\r\n",
        "open(folder_path + 'vocab.txt', 'w').write('\\n'.join(fasttext_vocab))\r\n",
        "# ghi file vector\r\n",
        "vec = np.array(vector)\r\n",
        "np.save(folder_path + 'vec.npy', vec)\r\n",
        "print('Finish write vocabulary and vector.')\r\n",
        "print('Vector shape: ',vec.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}